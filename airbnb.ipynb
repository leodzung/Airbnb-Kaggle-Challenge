{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Import Libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"import collections\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport datetime as dt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nfrom random import choice\nimport tensorflow as tf\nfrom sklearn.model_selection import GridSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Utilities"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# def remove_rare_values_inplace(df_frame, column_list, threshold):\n#     \"\"\" Remove rare values to speed up computation.\n#     Args:\n#         df_frame -- A pandas data frame.\n#         column_list -- A list of columns.\n#         threshold -- The threshold, below which a value is removed.\n#     \"\"\"\n#     insignificant_population = int(np.floor(threshold * len(df_frame)))\n#     for cat in column_list:\n#         freqs = collections.Counter(df_frame[cat])\n#         other = [i for i in freqs if freqs[i] < insignificant_population]\n#         for i in other:\n#             df_frame[cat].replace(i, 'other', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Feature Extraction"},{"metadata":{"trusted":true},"cell_type":"code","source":"# '''\n# Extracting freatures from the session data.\n# The data set can be found here:\n# www.kaggle.com/c/airbnb-recruiting-new-user-bookings/download/sessions.csv.zip\n# '''\n\n# INDEX_COLUMN = 'user_id'\n# SECS_ELAPSED_NUMERICAL = 'secs_elapsed'\n# CATEGORICAL_FEATURES = ['action', 'action_type', 'action_detail', 'device_type']\n# SESSSIONS_CSV_FILE = '/kaggle/input/airbnb-recruiting-new-user-bookings/sessions.csv.zip'\n# OUTPUT_TO_CSV_FILE = 'session_features.csv'  # Results will be saved here.\n\n# # A parameter to speed-up computation. Categorical values that appear\n# # less than the threshold will be removed.\n# VALUE_THRESHOLD = 0.005\n\n\n# def extract_frequency_counts(pd_frame, column_list):\n#     \"\"\" Extract frequency counts from pd_frame.\n#     For each index (that correspond to a user) this method will count the\n#     number of times that C == Ci, where C is a column in column_list, and Ci\n#     is a unique value of that column. The arg olumn_list is assumed\n#     to contain categorical columns.\n#     Args:\n#         df_frame -- A pandas data frame.\n#         column_list -- A list of columns.\n#     Returns:\n#         A pandas DataFrame, containing frequency counts.\n#     \"\"\"\n#     df_extracted_sessions = []\n#     for col in column_list:\n#         for val in set(pd_frame[col]):\n#             print('Extracting frequency counts for (%s == %s)' % (col, val))\n#             tmp_df = pd_frame.groupby(pd_frame.index).apply(\n#                 lambda group, x=col, y=val: np.sum(group[x] == y))\n#             tmp_df.name = '%s=%s' % (col, val)\n#             df_extracted_sessions.append(tmp_df)\n#     frequency_counts = pd.concat(df_extracted_sessions, axis=1)\n#     return frequency_counts\n\n\n# def extract_distribution_stats(pd_frame, numerical_col):\n#     \"\"\" Extract simple distribution statistics from a numerical column.\n#     Args:\n#         df_frame -- A pandas data frame.\n#         numerical_col -- A column in pd_frame that contains numerical values.\n#     Returns:\n#         A pandas DataFrame, containing simple satistics for col_name.\n#     \"\"\"\n#     tmp_df = pd_frame[numerical_col].groupby(pd_frame.index).aggregate(\n#         [np.mean, np.std, np.median, stats.skew])\n#     tmp_df.columns = ['%s_%s'% (numerical_col, i) for i in tmp_df.columns]\n#     return tmp_df\n\n\n# \"\"\"\n# Extract frequency counts from categorical columns and simple distribution\n# statistics from numerical ones.\n# \"\"\"\n# # Load basic training and testing data, from CSV file.\n# sessions = pd.read_csv(SESSSIONS_CSV_FILE)\n# sessions.set_index(INDEX_COLUMN, inplace=True)\n# sessions.fillna(-1, inplace=True)\n# # Extract features from sessions.\n# remove_rare_values_inplace(sessions, CATEGORICAL_FEATURES, VALUE_THRESHOLD)\n# frequency_counts = extract_frequency_counts(sessions, CATEGORICAL_FEATURES)\n# simple_stats = extract_distribution_stats(sessions, SECS_ELAPSED_NUMERICAL)\n# # Save new data.\n# session_data = pd.concat((frequency_counts, simple_stats), axis=1)\n# session_data.fillna(-1, inplace=True)\n# session_data.to_csv(OUTPUT_TO_CSV_FILE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"## Data Exploration"},{"metadata":{"trusted":true},"cell_type":"code","source":"TRAIN_DATA_BASIC = '/kaggle/input/airbnb-recruiting-new-user-bookings/train_users_2.csv.zip'\nTEST_DATA_BASIC = '/kaggle/input/airbnb-recruiting-new-user-bookings/test_users.csv.zip'\n\ntraining = pd.read_csv(TRAIN_DATA_BASIC, index_col=0)\ntesting = pd.read_csv(TEST_DATA_BASIC, index_col=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"We have\", training.shape[0], \"users in the training set and\", \n      testing.shape[0], \"in the test set.\")\nprint(\"In total we have\", training.shape[0] + testing.shape[0], \"users.\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge train and test users\nusers = pd.concat((training, testing), axis=0, ignore_index=True)\n\nusers.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Missing Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"users.gender.replace('-unknown-', np.nan, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users_nan = (users.isnull().sum() / users.shape[0]) * 100\nusers_nan[users_nan > 0].drop('country_destination')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Just for the sake of curiosity; we have\", \n      int((training.date_first_booking.isnull().sum() / training.shape[0]) * 100), \n      \"% of missing values at date_first_booking in the training data\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.age.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(sum(users.age > 122))\nprint(sum(users.age < 18))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users[users.age > 122]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users[users.age < 18]['age'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.loc[users.age > 95, 'age'] = np.nan\nusers.loc[users.age < 13, 'age'] = np.nan","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualization"},{"metadata":{"trusted":true},"cell_type":"code","source":"users.gender.value_counts(dropna=False).plot(kind='bar', color='#FD5C64', rot=0)\nplt.xlabel('Gender')\nsns.despine()\nplt.savefig('gender.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"women = sum(users['gender'] == 'FEMALE')\nmen = sum(users['gender'] == 'MALE')\n\nfemale_destinations = users.loc[users['gender'] == 'FEMALE', 'country_destination'].value_counts() / women * 100\nmale_destinations = users.loc[users['gender'] == 'MALE', 'country_destination'].value_counts() / men * 100\n\n# Bar width\nwidth = 0.4\n\nmale_destinations.plot(kind='bar', width=width, color='#4DD3C9', position=0, label='Male', rot=0)\nfemale_destinations.plot(kind='bar', width=width, color='#FFA35D', position=1, label='Female', rot=0)\n\nplt.legend()\nplt.xlabel('Destination Country')\nplt.ylabel('Percentage')\n\nsns.despine()\nplt.show()\nplt.savefig('country_by_gender.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"destination_percentage = users.country_destination.value_counts() / users.shape[0] * 100\ndestination_percentage.plot(kind='bar',color='#FD5C64', rot=0)\n# Using seaborn can also be plotted\n# sns.countplot(x=\"country_destination\", data=users, order=list(users.country_destination.value_counts().keys()))\nplt.xlabel('Destination Country')\nplt.ylabel('Percentage')\nsns.despine()\nplt.savefig('country_percentile.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(users.age.dropna(), color='#FD5C64')\nplt.xlabel('Age')\nsns.despine()\nplt.savefig('age.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"age = 45\n\nyounger = sum(users.loc[users['age'] < age, 'country_destination'].value_counts())\nolder = sum(users.loc[users['age'] > age, 'country_destination'].value_counts())\n\nyounger_destinations = users.loc[users['age'] < age, 'country_destination'].value_counts() / younger * 100\nolder_destinations = users.loc[users['age'] > age, 'country_destination'].value_counts() / older * 100\n\nyounger_destinations.plot(kind='bar', width=width, color='#63EA55', position=0, label='Youngers', rot=0)\nolder_destinations.plot(kind='bar', width=width, color='#4DD3C9', position=1, label='Olders', rot=0)\n\nplt.legend()\nplt.xlabel('Destination Country')\nplt.ylabel('Percentage')\n\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((sum(users.language == 'en') / users.shape[0])*100)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set_style(\"whitegrid\", {'axes.edgecolor': '0'})\nsns.set_context(\"poster\", font_scale=1.1)\nusers.date_account_created.value_counts().plot(kind='line', linewidth=1.2, color='#FD5C64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users['date_account_created'] = pd.to_datetime(users['date_account_created'])\nusers['date_first_booking'] = pd.to_datetime(users['date_first_booking'])\nusers['date_first_active'] = pd.to_datetime((users.timestamp_first_active // 1000000), format='%Y%m%d')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users.date_first_active.value_counts().plot(kind='line', linewidth=1.2, color='#FD5C64')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"users_2013 = users[users['date_first_active'] > pd.to_datetime(20130101, format='%Y%m%d')]\nusers_2013 = users_2013[users_2013['date_first_active'] < pd.to_datetime(20140101, format='%Y%m%d')]\nusers_2013.date_first_active.value_counts().plot(kind='line', linewidth=2, color='#FD5C64')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"weekdays = []\nfor date in users.date_account_created:\n    weekdays.append(date.weekday())\nweekdays = pd.Series(weekdays)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.barplot(x = weekdays.value_counts().index, y=weekdays.value_counts().values, order=range(0,7))\nplt.xlabel('Week Day')\nsns.despine()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"date = pd.to_datetime(20140101, format='%Y%m%d')\n\nbefore = sum(users.loc[users['date_first_active'] < date, 'country_destination'].value_counts())\nafter = sum(users.loc[users['date_first_active'] > date, 'country_destination'].value_counts())\nbefore_destinations = users.loc[users['date_first_active'] < date, \n                                'country_destination'].value_counts() / before * 100\nafter_destinations = users.loc[users['date_first_active'] > date, \n                               'country_destination'].value_counts() / after * 100\nbefore_destinations.plot(kind='bar', width=width, color='#63EA55', position=0, label='Before 2014', rot=0)\nafter_destinations.plot(kind='bar', width=width, color='#4DD3C9', position=1, label='After 2014', rot=0)\n\nplt.legend()\nplt.xlabel('Destination Country')\nplt.ylabel('Percentage')\n\nsns.despine()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Process Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# '''\n# Preparing the data for the classifiers.\n# '''\n\nLABEL = 'country_destination'\nCATEGORICAL_FEATURES = ['affiliate_channel', 'affiliate_provider',\n                        'first_affiliate_tracked', 'first_browser',\n                        'first_device_type', 'gender', 'language', 'signup_app',\n                        'signup_method', 'signup_flow']\n\nDATE_FORMAT = '%Y-%m-%d'                # Expected format for date.\nACCOUNT_DATE = 'date_account_created'   # Date column that will be exploited.\nACCOUNT_DATE_YEAR = '%s_%s' % (ACCOUNT_DATE, 'year')\nACCOUNT_DATE_MONTH = '%s_%s' % (ACCOUNT_DATE, 'month')\nUNUSED_DATE_COLUMNS = ['timestamp_first_active', 'date_first_booking']\n\n# TRAIN_DATA_BASIC = '/kaggle/input/airbnb-recruiting-new-user-bookings/train_users_2.csv.zip'\n# TEST_DATA_BASIC = '/kaggle/input/airbnb-recruiting-new-user-bookings/test_users.csv.zip'\nSESSION_DATA = '/kaggle/input/airbnb/session_features.csv'\nTRAINING_FINAL_CSV_FILE = '/kaggle/input/airbnb/training_features.csv'\nTESTING_FINAL_CSV_FILE = '/kaggle/input/airbnb/testing_features.csv'\nLABELS_FINAL_CSV_FILE = '/kaggle/input/airbnb/labels.csv'\n\n# # A parameter to speed-up computation. Categorical values that appear\n# # less than the threshold will be removed.\n# VALUE_THRESHOLD = 0.001\n\n\n# def _parse_date(date_str, format_str):\n#     \"\"\" Extract features from the data_account_creted column.\n#     Warning: There is strong dependency between this method and the method\n#     replace_dates_inplace.\n#     Args:\n#         date_str -- A string containing a date value.\n#         str_format -- The format of the string date.\n#     Returns:\n#         A list of 4 values containing the extracted [year, month, day, weekend].\n#     \"\"\"\n#     time_dt = dt.datetime.strptime(date_str, format_str)\n#     return [time_dt.year, time_dt.month, time_dt.day, time_dt.weekday()]\n\n\n# def extract_dates_inplace(features, date_column):\n#     \"\"\" Extract from the date-columns, year, month, and other numericals.\n#     Warning: There is strong dependency between this method and _parse_date.\n#     \"\"\"\n#     extracted_vals = np.vstack(features[date_column].apply(\n#         (lambda x: _parse_date(x, DATE_FORMAT))))\n#     for i, period in enumerate(['year', 'month', 'day', 'weekday']):\n#         features['%s_%s' % (date_column, period)] = extracted_vals[:, i]\n#     features.drop(date_column, inplace=True, axis=1)\n\n\ndef apply_one_hot_encoding(pd_frame, column_list):\n    \"\"\" Apply One-Hot-Encoding to pd_frame's categorical columns.\n    Args:\n        df_frame -- A pandas data frame.\n        column_list -- A list of categorical columns, in df_frame.\n    Returns:\n        A pandas dataframe where the colums in column_list have been replaced\n            by one-hot-encoded-columns.\n    \"\"\"\n    new_column_list = []\n    for col in column_list:\n        tmp = pd.get_dummies(pd_frame[col], prefix=col)\n        new_column_list.append(tmp)\n    new_pd_frame = pd.concat(new_column_list+[pd_frame], axis=1)\n    new_pd_frame.drop(column_list, inplace=True, axis=1)\n    return new_pd_frame\n\n\n# def get_basic_train_test_data():\n#     \"\"\" Load the basic data in a pandas dataframe, and pre-process them. \"\"\"\n#     training = pd.read_csv(TRAIN_DATA_BASIC, index_col=0)\n#     testing = pd.read_csv(TEST_DATA_BASIC, index_col=0)\n#     labels = training[LABEL].copy()\n#     training.drop(LABEL, inplace=True, axis=1)\n#     features = pd.concat((training, testing), axis=0)\n#     features.fillna(-1, inplace=True)\n\n#     # Process all features by removing rare values, appling one-hot-encoding to\n#     # those that are categorical and extracting numericals from ACCOUNT_DATE.\n\n#     remove_rare_values_inplace(features, CATEGORICAL_FEATURES, VALUE_THRESHOLD)\n#     features = apply_one_hot_encoding(features, CATEGORICAL_FEATURES)\n#     extract_dates_inplace(features, ACCOUNT_DATE)\n#     features.drop(UNUSED_DATE_COLUMNS, inplace=True, axis=1)\n#     return features, labels, training.index, testing.index\n\n\n# \"\"\" Load basic data, add session data, and prepare them for predition. \"\"\"\n# features, labels, training_ids, testing_ids = get_basic_train_test_data()\n# sessions = pd.read_csv(SESSION_DATA, index_col=0)\n# features = pd.concat((features, sessions), axis=1)\n# features.fillna(-1, inplace=True)\n# # Save data training and testing data.\n# training = features.ix[training_ids]\n# testing = features.ix[testing_ids]\n\n# # Warning: When saving the data, it's important that the header is True,\n# # because labels is of type pandas.core.series.Series, while training is of\n# # type pandas.core.frame.DataFrame, and they have different default values\n# # for the header argument.\n\n# assert set(training.index) == set(labels.index)\n# training.to_csv(TRAINING_FINAL_CSV_FILE, header=True)\n# testing.to_csv(TESTING_FINAL_CSV_FILE, header=True)\n# labels.to_csv(LABELS_FINAL_CSV_FILE, header=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Building"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Base_Model(object):\n    \n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n        self.train_df = train_df\n        self.test_df = test_df\n        self.features = features\n        self.n_splits = n_splits\n        self.categoricals = categoricals\n        self.target = LABEL\n        self.cv = self.get_cv()\n        self.verbose = verbose\n        self.params = self.get_params()\n        self.y_pred, self.model = self.fit()\n        \n    def train_model(self, train_set, val_set):\n        raise NotImplementedError\n        \n    def get_cv(self):\n        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n        return cv.split(self.train_df, self.train_df[self.target])\n    \n    def get_params(self):\n        raise NotImplementedError\n        \n    def get_model(self):\n        return self.model\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        raise NotImplementedError\n        \n    def convert_x(self, x):\n        return x\n        \n    def fit(self):\n#         oof_pred = np.zeros((len(train_df), ))\n        y_pred = np.zeros((len(test_df), len(set(label))))\n        for fold, (train_idx, val_idx) in enumerate(self.cv):\n            print('--------------------------------------- FOLD %d -----------------------------------------' %fold)\n            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n            \n            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n            model = self.train_model(train_set, val_set)\n            \n#             conv_x_val = self.convert_x(x_val)\n#             oof_pred = model.predict(conv_x_val)\n            \n            x_test = self.convert_x(self.test_df[self.features])\n            y_pred += model.predict_proba(x_test) / self.n_splits\n            \n        return y_pred, model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Xgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 3 if self.verbose else 0\n        clf = xgb.XGBClassifier(self.get_params())\n        clf.fit(train_set['X'], \n                train_set['y'], \n                eval_set=[(val_set['X'], val_set['y'])])\n        return clf\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {\n            'max_bin': 63,\n            'colsample_bytree': 0.8,                 \n            'learning_rate': 0.1,\n            'max_depth': 7,\n            'subsample': 0.7,\n            'objective':'multi:softprob',\n            #'eval_metric':'rmse',\n            'min_child_weight':3,\n            'gamma':0.25,\n            'n_estimators':5000,\n            'tree_method':'gpu_hist'}\n\n        return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Lgb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        params = self.get_params()\n        clf = lgb.LGBMClassifier(max_bin = 100,\n                                 device = 'gpu',\n                                 num_leaves = 30,\n                                 n_estimators = params['n_estimators'],\n                                  boosting_type = params['boosting_type'],\n                                  objective = params['objective'],\n                                  subsample = params['subsample'],\n                                  subsample_freq = params['subsample_freq'],\n                                  learning_rate = params['learning_rate'],\n                                  max_depth = params['max_depth'],\n                                  early_stopping_rounds = params['early_stopping_rounds'])\n        \n        return clf.fit(train_set['X'], \n                train_set['y'], \n                eval_set=[(val_set['X'], val_set['y'])])\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {\n                    'n_estimators':1000,\n                    'boosting_type': 'gbdt',\n                    'objective': 'multiclass',\n                    'eval_metric': 'logloss',\n                    'subsample': 0.7,\n                    'subsample_freq': 1,\n                    'learning_rate': 0.1,\n                    'feature_fraction': 0.9,\n                    'max_depth': 7,\n                    'early_stopping_rounds': 100\n                    }\n        return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Catb_Model(Base_Model):\n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n        params = self.get_params()\n        clf = CatBoostClassifier(learning_rate = params['learning_rate'],\n                                  task_type = params['task_type'],\n                                  iterations = params['iterations'],\n                                  od_type = params['od_type'],\n                                  depth = params['depth'],\n                                  colsample_bylevel = params['colsample_bylevel'],\n                                  l2_leaf_reg = params['l2_leaf_reg'],\n                                  random_seed = params['random_seed'],\n                                  use_best_model = params['use_best_model'])\n        clf.fit(train_set['X'], \n                train_set['y'], \n                eval_set=(val_set['X'], val_set['y']),\n                verbose=verbosity, \n                cat_features=self.categoricals)\n        return clf\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        params = {'learning_rate': 0.1,\n                   'task_type': \"CPU\",\n                   'iterations':1000,\n                   'od_type': \"Iter\",\n                    'depth': 10,\n                  'colsample_bylevel': 0.5, \n                   'early_stopping_rounds': 300,\n                    'l2_leaf_reg': 18,\n                   'random_seed': 42,\n                    'use_best_model': True\n                    }\n        return params","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Cnn_Model(Base_Model):\n    \n    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n        features = features.copy()\n        if len(categoricals) > 0:\n            for cat in categoricals:\n                enc = OneHotEncoder()\n                train_cats = enc.fit_transform(train_df[[cat]])\n                test_cats = enc.transform(test_df[[cat]])\n                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n                features += cat_cols\n                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n                train_df = pd.concat([train_df, train_cats], axis=1)\n                test_df = pd.concat([test_df, test_cats], axis=1)\n        scaler = MinMaxScaler()\n        train_df[features] = scaler.fit_transform(train_df[features])\n        test_df[features] = scaler.transform(test_df[features])\n        self.create_feat_2d(features)\n        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n        \n    def create_feat_2d(self, features, n_feats_repeat=50):\n        self.n_feats = len(features)\n        self.n_feats_repeat = n_feats_repeat\n        self.mask = np.zeros((self.n_feats_repeat, self.n_feats), dtype=np.int32)\n        for i in range(self.n_feats_repeat):\n            l = list(range(self.n_feats))\n            for j in range(self.n_feats):\n                c = l.pop(choice(range(len(l))))\n                self.mask[i, j] = c\n        self.mask = tf.convert_to_tensor(self.mask)\n        print(self.mask.shape)\n        \n    \n    def train_model(self, train_set, val_set):\n        verbosity = 100 if self.verbose else 0\n\n        inp = tf.keras.layers.Input(shape=(self.n_feats))\n        x = tf.keras.layers.Lambda(lambda x: tf.gather(x, self.mask, axis=1))(inp)\n        x = tf.keras.layers.Reshape((self.n_feats_repeat, self.n_feats, 1))(x)\n        x = tf.keras.layers.Conv2D(18, (50, 50), strides=50, activation='relu')(x)\n        x = tf.keras.layers.Flatten()(x)\n        #x = tf.keras.layers.Dense(200, activation='relu')(x)\n        #x = tf.keras.layers.LayerNormalization()(x)\n        #x = tf.keras.layers.Dropout(0.3)(x)\n        x = tf.keras.layers.Dense(100, activation='relu')(x)\n        x = tf.keras.layers.LayerNormalization()(x)\n        x = tf.keras.layers.Dropout(0.3)(x)\n        x = tf.keras.layers.Dense(50, activation='relu')(x)\n        x = tf.keras.layers.LayerNormalization()(x)\n        x = tf.keras.layers.Dropout(0.3)(x)\n        out = tf.keras.layers.Dense(len(set(label)))(x)\n        \n        model = tf.keras.Model(inp, out)\n    \n        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy')\n        print(model.summary())\n        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n        model.fit(train_set['X'], \n                train_set['y'], \n                validation_data=(val_set['X'], val_set['y']),\n                epochs=100,\n                 callbacks=[save_best, early_stop])\n        model.load_weights('nn_model.w8')\n        self.model = model\n        return self\n    \n    def predict_proba(self, x_test):\n        return self.model.predict(x_test)\n        \n    def convert_dataset(self, x_train, y_train, x_val, y_val):\n        train_set = {'X': x_train, 'y': y_train}\n        val_set = {'X': x_val, 'y': y_val}\n        return train_set, val_set\n        \n    def get_params(self):\n        return None","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(TRAINING_FINAL_CSV_FILE, index_col=0)\ntest_df = pd.read_csv(TESTING_FINAL_CSV_FILE, index_col=0)\nlabel_df = pd.read_csv(LABELS_FINAL_CSV_FILE, index_col=0)\n\nfeatures = train_df.columns\n\nencoder = LabelEncoder()\nencoder.fit(label_df[LABEL])\nlabel = encoder.transform(label_df[LABEL])\ntrain_df['country_destination'] = label\ntrain_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gridParams = {\n#     'learning_rate': [0.05, 0.1, 0.2],\n#     'num_iterations':[100, 200, 500],\n#     'num_leaves': [50, 70, 100],\n#     'subsample' : [0.7, 0.75]\n# }\n\n# params = {\n#                     'n_estimators':5000,\n#                     'boosting_type': 'gbdt',\n#                     'objective': 'multiclass',\n#                     'eval_metric': 'logloss',\n#                     'subsample': 0.7,\n#                     'subsample_freq': 1,\n#                     'learning_rate': 0.1,\n#                     'feature_fraction': 0.9,\n#                     'max_depth': 7,\n#                     'lambda_l1': 1,  \n#                     'lambda_l2': 1,\n#                     'early_stopping_rounds': 100\n# }\n\n# clf = lgb.LGBMClassifier(max_bin = 100,\n#                                  num_iterations = 200,\n#                                  device = 'gpu',\n#                                  gpu_platform_id = 0,\n#                                  gpu_device_id = 0,\n#                                  num_leaves = 70,\n#                                   boosting_type = params['boosting_type'],\n#                                   objective = params['objective'],\n#                                   eval_metric = params['eval_metric'],\n#                                   subsample = params['subsample'],\n#                                   subsample_freq = params['subsample_freq'],\n#                                   learning_rate = params['learning_rate'],\n#                                   feature_fraction = params['feature_fraction'],\n#                                   max_depth = params['max_depth'],\n#                                   lambda_l1 = params['lambda_l1'],\n#                                   lambda_l2 = params['lambda_l2'])\n\n# grid = GridSearchCV(clf, gridParams,\n#                     verbose=2,\n#                     cv=5,\n#                     n_jobs=-1)\n# # Run the grid\n# grid.fit(train_df[features], train_df[LABEL])\n\n# print(grid.best_params_)\n# print(grid.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"LGB_WEIGHT = 0.4\nLGB_FRESH_WEIGHT = 0.6\nXGB_WEIGHT = 0.0\nXGB_FRESH_WEIGHT = 0.0\nCAT_WEIGHT = 0.0\nCAT_FRESH_WEIGHT = 0.0\nCNN_WEIGHT = 0.0\nCNN_FRESH_WEIGHT = 0.0\nFRESH_DATA_YEAR = 2014                   # Year when data is considered fresh.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if LGB_WEIGHT:\n    lgb_model = Lgb_Model(train_df, test_df, features=features, verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb_model.get_model()\n\nfeature_imp = pd.DataFrame(sorted(zip(model.feature_importances_,features)), columns=['Value','Feature'])\n\nTOP10 = 10\n\nplt.figure(figsize=(20, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(TOP10))\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()\nplt.savefig('top_10_features.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if LGB_FRESH_WEIGHT:\n    lgb_fresh_model = Lgb_Model(train_df[train_df[ACCOUNT_DATE_YEAR] >= FRESH_DATA_YEAR], test_df, features=features, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = lgb_fresh_model.get_model()\nmodel","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if XGB_WEIGHT:\n    xgb_model = Xgb_Model(train_df, test_df, features=features, verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if XGB_FRESH_WEIGHT:\n    xgb_fresh_model = Xgb_Model(train_df[train_df[ACCOUNT_DATE_YEAR] >= FRESH_DATA_YEAR], test_df, features=features, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if CAT_WEIGHT:\n    cat_model = Catb_Model(train_df, test_df, features=features, verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if CAT_FRESH_WEIGHT:\n    cat_fresh_model = Catb_Model(train_df[train_df[ACCOUNT_DATE_YEAR] >= FRESH_DATA_YEAR], test_df, features=features, verbose=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if CNN_WEIGHT:\n    cnn_model = Cnn_Model(train_df, test_df, features=features, verbose=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"raw","source":"FRESH_DATA_YEAR = 1                   # Year when data is considered fresh, after normalize\nif CNN_FRESH_WEIGHT:\n    cnn_fresh_model = Cnn_Model(train_df[train_df[ACCOUNT_DATE_YEAR] >= FRESH_DATA_YEAR], test_df, features=features, verbose=True)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# label_df['country_destination'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# predictions[0][np.argsort(predictions[0])[::-1]][4]\n# # predictions[0] * 100","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble"},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions = np.zeros((len(test_df), len(set(label))))\n\nif LGB_WEIGHT:\n    predictions += LGB_WEIGHT * lgb_model.y_pred\nif LGB_FRESH_WEIGHT:\n    predictions += LGB_FRESH_WEIGHT * lgb_fresh_model.y_pred\nif XGB_WEIGHT:\n    predictions += XGB_WEIGHT * xgb_model.y_pred\nif XGB_FRESH_WEIGHT:\n    predictions += XGB_FRESH_WEIGHT * xgb_fresh_model.y_pred\nif CAT_WEIGHT:\n    predictions += CAT_WEIGHT * cat_model.y_pred\nif CAT_FRESH_WEIGHT:\n    predictions += CAT_FRESH_WEIGHT * cat_fresh_model.y_pred\nif CNN_WEIGHT:\n    predictions += CNN_WEIGHT * cnn_model.y_pred\nif CNN_FRESH_WEIGHT:\n    predictions += CNN_FRESH_WEIGHT * cnn_fresh_model.y_pred","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"# '''\n# Predict Users' new booking by combining of RnadomForest and XGB classifiers.\n# '''\n# DEPTH_XGB, ESTIMATORS_XGB, LEARNING_XGB, SUBSAMPLE_XGB, COLSAMPLE_XGB = (\n#     7, 60, 0.2, 0.7, 0.6)                # XGBoost parameters.\n\n# ESTIMATORS_RF, CRITERION_RF, DEPTH_RF, MIN_LEAF_RF, JOBS_RF = (\n#     500, 'gini', 20, 8, 30)              # RandomForestClassifier parameters.\n# FRESH_DATA_YEAR = 2014                   # Year when data is considered fresh.\n\n# # Tunning ensemble members. The votes show the importnce of each classfier\n# # in the final prediction.\n\n# XGB_ALL_VOTE, RF_ALL_VOTE, XGB_FRESH_VOTE, RF_FRESH_VOTE = (1, 1, 1, 1)\n\n\n# def perform_prediction(training, labels, testing, xgb_votes, rf_votes):\n#     \"\"\" Perform prediction using a combination of XGB and RandomForests. \"\"\"\n#     predictions = np.zeros((len(testing), len(set(labels))))\n#     # Predictions using xgboost.\n#     for i in range(xgb_votes):\n#         print('XGB vote %d' % i)\n#         clf = xgb.XGBClassifier(\n#             max_depth=DEPTH_XGB, learning_rate=LEARNING_XGB,\n#             n_estimators=ESTIMATORS_XGB, objective='multi:softprob',\n#             subsample=SUBSAMPLE_XGB, colsample_bytree=COLSAMPLE_XGB)\n#         clf.fit(training, labels, verbose=True)\n#         prediction = clf.predict_proba(testing)\n#         print(prediction)\n#         predictions += prediction\n#     # Predictions using RandomForestClassifier.\n# #     for i in range(rf_votes):\n# #         print('RandomForest vote %d' % i)\n# #         rand_forest = RandomForestClassifier(\n# #             n_estimators=ESTIMATORS_RF, criterion=CRITERION_RF, n_jobs=JOBS_RF,\n# #             max_depth=DEPTH_RF, min_samples_leaf=MIN_LEAF_RF, bootstrap=True)\n# #         rand_forest.fit(training, labels)\n# #         predictions += rand_forest.predict_proba(testing)\n#     return predictions\n\n# \"\"\" Perform prediction. \"\"\"\n# train_df = pd.read_csv(TRAINING_FINAL_CSV_FILE, index_col=0)\n# labels_df = pd.read_csv(LABELS_FINAL_CSV_FILE, index_col=0)\n# test_df = pd.read_csv(TESTING_FINAL_CSV_FILE, index_col=0)\n# assert set(train_df.index) == set(labels_df.index)\n\n# encoder = LabelEncoder()\n# encoder.fit(labels_df[LABEL])\n# predictions = np.zeros((len(test_df), len(encoder.classes_)))\n\n# # Use the full data set for the prediction.\n# labels = encoder.transform(labels_df[LABEL])\n# predictions += perform_prediction(\n#     train_df, labels, test_df, XGB_ALL_VOTE, RF_ALL_VOTE)\n\n# # Use only \"fresh\" data for prediction. Fresh data, are considered those\n# # that are an ACCOUNT_DATE_YEAR equal or higher than FRESH_DATA_YEAR.\n\n# train_fresh = train_df[train_df[ACCOUNT_DATE_YEAR] >= FRESH_DATA_YEAR]\n# labels_fresh = encoder.transform(labels_df.ix[train_fresh.index][LABEL])\n# predictions += perform_prediction(\n#     train_fresh, labels_fresh, test_df, XGB_FRESH_VOTE, RF_FRESH_VOTE)\n\n# print(\"Prediction shape %d %d\", predictions.shape[0], predictions.shape[1])\n\n# # Use the 5 classes with highest scores.\n# ids, countries = ([], [])\n# for i in range(len(test_df)):\n#     idx = test_df.index[i]\n#     ids += [idx] * 5\n#     countries += encoder.inverse_transform(\n#         np.argsort(predictions[i])[::-1])[:5].tolist()\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use the 5 classes with highest scores.\nNUM_OF_CLASSES = 5\nids, countries = ([], [])\n\nTOP_5 = ['NDF', 'US', 'other', 'FR', 'IT']\ncount = 0\n\nfor i in range(len(test_df)):\n    print('--------------------------------------- i %i -----------------------------------------' %i)\n    idx = test_df.index[i]\n    ids += [idx] * NUM_OF_CLASSES\n    \n#     print(encoder.inverse_transform(\n#         np.argsort(predictions[i])[::-1])[:5].tolist())\n    top_6 = encoder.inverse_transform(\n        np.argsort(predictions[i])[::-1])[:NUM_OF_CLASSES + 1].tolist()\n#     print(top_6)\n#     countries += encoder.inverse_transform(\n#         np.argsort(predictions[i])[::-1])[:NUM_OF_CLASSES + 1].tolist()\n    \n    fifth = predictions[i][np.argsort(predictions[i])[::-1]][NUM_OF_CLASSES - 1]\n    sixth = predictions[i][np.argsort(predictions[i])[::-1]][NUM_OF_CLASSES]\n#     print(predictions[i][np.argsort(predictions[i])[::-1]][NUM_OF_CLASSES - 1])\n#     print(predictions[i][np.argsort(predictions[i])[::-1]][NUM_OF_CLASSES])\n\n    if top_6[NUM_OF_CLASSES] in TOP_5 and top_6[NUM_OF_CLASSES - 1] not in TOP_5 and (fifth-sixth) < 0.002:\n        top_6.remove(top_6[NUM_OF_CLASSES - 1])\n        count += 1\n    else:\n        top_6.remove(top_6[NUM_OF_CLASSES])\n    print(top_6)    \n    \n    countries += top_6\n    \nprint(count)\nprint(count/len(test_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"SUBMISSION_CSV = 'submission.csv'  # Where to store the predictions.\n\n# Save prediction in CSV file.\nsub = pd.DataFrame(\n    np.column_stack((ids, countries)), columns=['id', 'country'])\nprint(sub.shape)\nsub.to_csv(SUBMISSION_CSV, index=False)\nsub","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}