{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import datetime as dt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "from random import choice\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# def remove_rare_values_inplace(df_frame, column_list, threshold):\n",
    "#     \"\"\" Remove rare values to speed up computation.\n",
    "#     Args:\n",
    "#         df_frame -- A pandas data frame.\n",
    "#         column_list -- A list of columns.\n",
    "#         threshold -- The threshold, below which a value is removed.\n",
    "#     \"\"\"\n",
    "#     insignificant_population = int(np.floor(threshold * len(df_frame)))\n",
    "#     for cat in column_list:\n",
    "#         freqs = collections.Counter(df_frame[cat])\n",
    "#         other = [i for i in freqs if freqs[i] < insignificant_population]\n",
    "#         for i in other:\n",
    "#             df_frame[cat].replace(i, 'other', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Extracting freatures from the session data.\n",
    "# The data set can be found here:\n",
    "# www.kaggle.com/c/airbnb-recruiting-new-user-bookings/download/sessions.csv.zip\n",
    "# '''\n",
    "\n",
    "# INDEX_COLUMN = 'user_id'\n",
    "# SECS_ELAPSED_NUMERICAL = 'secs_elapsed'\n",
    "# CATEGORICAL_FEATURES = ['action', 'action_type', 'action_detail', 'device_type']\n",
    "# SESSSIONS_CSV_FILE = '/kaggle/input/airbnb-recruiting-new-user-bookings/sessions.csv.zip'\n",
    "# OUTPUT_TO_CSV_FILE = 'session_features.csv'  # Results will be saved here.\n",
    "\n",
    "# # A parameter to speed-up computation. Categorical values that appear\n",
    "# # less than the threshold will be removed.\n",
    "# VALUE_THRESHOLD = 0.005\n",
    "\n",
    "\n",
    "# def extract_frequency_counts(pd_frame, column_list):\n",
    "#     \"\"\" Extract frequency counts from pd_frame.\n",
    "#     For each index (that correspond to a user) this method will count the\n",
    "#     number of times that C == Ci, where C is a column in column_list, and Ci\n",
    "#     is a unique value of that column. The arg olumn_list is assumed\n",
    "#     to contain categorical columns.\n",
    "#     Args:\n",
    "#         df_frame -- A pandas data frame.\n",
    "#         column_list -- A list of columns.\n",
    "#     Returns:\n",
    "#         A pandas DataFrame, containing frequency counts.\n",
    "#     \"\"\"\n",
    "#     df_extracted_sessions = []\n",
    "#     for col in column_list:\n",
    "#         for val in set(pd_frame[col]):\n",
    "#             print('Extracting frequency counts for (%s == %s)' % (col, val))\n",
    "#             tmp_df = pd_frame.groupby(pd_frame.index).apply(\n",
    "#                 lambda group, x=col, y=val: np.sum(group[x] == y))\n",
    "#             tmp_df.name = '%s=%s' % (col, val)\n",
    "#             df_extracted_sessions.append(tmp_df)\n",
    "#     frequency_counts = pd.concat(df_extracted_sessions, axis=1)\n",
    "#     return frequency_counts\n",
    "\n",
    "\n",
    "# def extract_distribution_stats(pd_frame, numerical_col):\n",
    "#     \"\"\" Extract simple distribution statistics from a numerical column.\n",
    "#     Args:\n",
    "#         df_frame -- A pandas data frame.\n",
    "#         numerical_col -- A column in pd_frame that contains numerical values.\n",
    "#     Returns:\n",
    "#         A pandas DataFrame, containing simple satistics for col_name.\n",
    "#     \"\"\"\n",
    "#     tmp_df = pd_frame[numerical_col].groupby(pd_frame.index).aggregate(\n",
    "#         [np.mean, np.std, np.median, stats.skew])\n",
    "#     tmp_df.columns = ['%s_%s'% (numerical_col, i) for i in tmp_df.columns]\n",
    "#     return tmp_df\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Extract frequency counts from categorical columns and simple distribution\n",
    "# statistics from numerical ones.\n",
    "# \"\"\"\n",
    "# # Load basic training and testing data, from CSV file.\n",
    "# sessions = pd.read_csv(SESSSIONS_CSV_FILE)\n",
    "# sessions.set_index(INDEX_COLUMN, inplace=True)\n",
    "# sessions.fillna(-1, inplace=True)\n",
    "# # Extract features from sessions.\n",
    "# remove_rare_values_inplace(sessions, CATEGORICAL_FEATURES, VALUE_THRESHOLD)\n",
    "# frequency_counts = extract_frequency_counts(sessions, CATEGORICAL_FEATURES)\n",
    "# simple_stats = extract_distribution_stats(sessions, SECS_ELAPSED_NUMERICAL)\n",
    "# # Save new data.\n",
    "# session_data = pd.concat((frequency_counts, simple_stats), axis=1)\n",
    "# session_data.fillna(-1, inplace=True)\n",
    "# session_data.to_csv(OUTPUT_TO_CSV_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Preparing the data for the classifiers.\n",
    "# '''\n",
    "\n",
    "LABEL = 'country_destination'\n",
    "CATEGORICAL_FEATURES = ['affiliate_channel', 'affiliate_provider',\n",
    "                        'first_affiliate_tracked', 'first_browser',\n",
    "                        'first_device_type', 'gender', 'language', 'signup_app',\n",
    "                        'signup_method', 'signup_flow']\n",
    "\n",
    "DATE_FORMAT = '%Y-%m-%d'                # Expected format for date.\n",
    "ACCOUNT_DATE = 'date_account_created'   # Date column that will be exploited.\n",
    "ACCOUNT_DATE_YEAR = '%s_%s' % (ACCOUNT_DATE, 'year')\n",
    "ACCOUNT_DATE_MONTH = '%s_%s' % (ACCOUNT_DATE, 'month')\n",
    "UNUSED_DATE_COLUMNS = ['timestamp_first_active', 'date_first_booking']\n",
    "\n",
    "# TRAIN_DATA_BASIC = '/kaggle/input/airbnb-recruiting-new-user-bookings/train_users_2.csv.zip'\n",
    "# TEST_DATA_BASIC = '/kaggle/input/airbnb-recruiting-new-user-bookings/test_users.csv.zip'\n",
    "SESSION_DATA = './session_features.csv'\n",
    "TRAINING_FINAL_CSV_FILE = './training_features.csv'\n",
    "TESTING_FINAL_CSV_FILE = './testing_features.csv'\n",
    "LABELS_FINAL_CSV_FILE = './labels.csv'\n",
    "\n",
    "# # A parameter to speed-up computation. Categorical values that appear\n",
    "# # less than the threshold will be removed.\n",
    "# VALUE_THRESHOLD = 0.001\n",
    "\n",
    "\n",
    "# def _parse_date(date_str, format_str):\n",
    "#     \"\"\" Extract features from the data_account_creted column.\n",
    "#     Warning: There is strong dependency between this method and the method\n",
    "#     replace_dates_inplace.\n",
    "#     Args:\n",
    "#         date_str -- A string containing a date value.\n",
    "#         str_format -- The format of the string date.\n",
    "#     Returns:\n",
    "#         A list of 4 values containing the extracted [year, month, day, weekend].\n",
    "#     \"\"\"\n",
    "#     time_dt = dt.datetime.strptime(date_str, format_str)\n",
    "#     return [time_dt.year, time_dt.month, time_dt.day, time_dt.weekday()]\n",
    "\n",
    "\n",
    "# def extract_dates_inplace(features, date_column):\n",
    "#     \"\"\" Extract from the date-columns, year, month, and other numericals.\n",
    "#     Warning: There is strong dependency between this method and _parse_date.\n",
    "#     \"\"\"\n",
    "#     extracted_vals = np.vstack(features[date_column].apply(\n",
    "#         (lambda x: _parse_date(x, DATE_FORMAT))))\n",
    "#     for i, period in enumerate(['year', 'month', 'day', 'weekday']):\n",
    "#         features['%s_%s' % (date_column, period)] = extracted_vals[:, i]\n",
    "#     features.drop(date_column, inplace=True, axis=1)\n",
    "\n",
    "\n",
    "def apply_one_hot_encoding(pd_frame, column_list):\n",
    "    \"\"\" Apply One-Hot-Encoding to pd_frame's categorical columns.\n",
    "    Args:\n",
    "        df_frame -- A pandas data frame.\n",
    "        column_list -- A list of categorical columns, in df_frame.\n",
    "    Returns:\n",
    "        A pandas dataframe where the colums in column_list have been replaced\n",
    "            by one-hot-encoded-columns.\n",
    "    \"\"\"\n",
    "    new_column_list = []\n",
    "    for col in column_list:\n",
    "        tmp = pd.get_dummies(pd_frame[col], prefix=col)\n",
    "        new_column_list.append(tmp)\n",
    "    new_pd_frame = pd.concat(new_column_list+[pd_frame], axis=1)\n",
    "    new_pd_frame.drop(column_list, inplace=True, axis=1)\n",
    "    return new_pd_frame\n",
    "\n",
    "\n",
    "# def get_basic_train_test_data():\n",
    "#     \"\"\" Load the basic data in a pandas dataframe, and pre-process them. \"\"\"\n",
    "#     training = pd.read_csv(TRAIN_DATA_BASIC, index_col=0)\n",
    "#     testing = pd.read_csv(TEST_DATA_BASIC, index_col=0)\n",
    "#     labels = training[LABEL].copy()\n",
    "#     training.drop(LABEL, inplace=True, axis=1)\n",
    "#     features = pd.concat((training, testing), axis=0)\n",
    "#     features.fillna(-1, inplace=True)\n",
    "\n",
    "#     # Process all features by removing rare values, appling one-hot-encoding to\n",
    "#     # those that are categorical and extracting numericals from ACCOUNT_DATE.\n",
    "\n",
    "#     remove_rare_values_inplace(features, CATEGORICAL_FEATURES, VALUE_THRESHOLD)\n",
    "#     features = apply_one_hot_encoding(features, CATEGORICAL_FEATURES)\n",
    "#     extract_dates_inplace(features, ACCOUNT_DATE)\n",
    "#     features.drop(UNUSED_DATE_COLUMNS, inplace=True, axis=1)\n",
    "#     return features, labels, training.index, testing.index\n",
    "\n",
    "\n",
    "# \"\"\" Load basic data, add session data, and prepare them for predition. \"\"\"\n",
    "# features, labels, training_ids, testing_ids = get_basic_train_test_data()\n",
    "# sessions = pd.read_csv(SESSION_DATA, index_col=0)\n",
    "# features = pd.concat((features, sessions), axis=1)\n",
    "# features.fillna(-1, inplace=True)\n",
    "# # Save data training and testing data.\n",
    "# training = features.ix[training_ids]\n",
    "# testing = features.ix[testing_ids]\n",
    "\n",
    "# # Warning: When saving the data, it's important that the header is True,\n",
    "# # because labels is of type pandas.core.series.Series, while training is of\n",
    "# # type pandas.core.frame.DataFrame, and they have different default values\n",
    "# # for the header argument.\n",
    "\n",
    "# assert set(training.index) == set(labels.index)\n",
    "# training.to_csv(TRAINING_FINAL_CSV_FILE, header=True)\n",
    "# testing.to_csv(TESTING_FINAL_CSV_FILE, header=True)\n",
    "# labels.to_csv(LABELS_FINAL_CSV_FILE, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.categoricals = categoricals\n",
    "        self.target = LABEL\n",
    "        self.cv = self.get_cv()\n",
    "        self.verbose = verbose\n",
    "        self.params = self.get_params()\n",
    "        self.y_pred, self.model = self.fit()\n",
    "        \n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def get_cv(self):\n",
    "        cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
    "        return cv.split(self.train_df, self.train_df[self.target])\n",
    "    \n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "        \n",
    "    def fit(self):\n",
    "#         oof_pred = np.zeros((len(train_df), ))\n",
    "        y_pred = np.zeros((len(test_df), len(set(label))))\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv):\n",
    "            print('--------------------------------------- FOLD %d -----------------------------------------' %fold)\n",
    "            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n",
    "            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n",
    "            \n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            model = self.train_model(train_set, val_set)\n",
    "            \n",
    "#             conv_x_val = self.convert_x(x_val)\n",
    "#             oof_pred = model.predict(conv_x_val)\n",
    "            \n",
    "            x_test = self.convert_x(self.test_df[self.features])\n",
    "            y_pred += model.predict_proba(x_test) / self.n_splits\n",
    "            \n",
    "        return y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xgb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 3 if self.verbose else 0\n",
    "        clf = xgb.XGBClassifier(self.get_params())\n",
    "        clf.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                eval_set=[(val_set['X'], val_set['y'])])\n",
    "        return clf\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {\n",
    "            'max_bin': 63,\n",
    "            'colsample_bytree': 0.8,                 \n",
    "            'learning_rate': 0.1,\n",
    "            'max_depth': 7,\n",
    "            'subsample': 0.7,\n",
    "            'objective':'multi:softprob',\n",
    "            #'eval_metric':'rmse',\n",
    "            'min_child_weight':3,\n",
    "            'gamma':0.25,\n",
    "            'n_estimators':5000,\n",
    "            'tree_method':'gpu_hist'}\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lgb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        params = self.get_params()\n",
    "        clf = lgb.LGBMClassifier(max_bin = 100,\n",
    "                                 num_iterations = 200,\n",
    "                                 device = 'gpu',\n",
    "                                 gpu_platform_id = 0,\n",
    "                                 gpu_device_id = 0,\n",
    "                                 num_leaves = 70,\n",
    "                                  boosting_type = params['boosting_type'],\n",
    "                                  objective = params['objective'],\n",
    "                                  eval_metric = params['eval_metric'],\n",
    "                                  subsample = params['subsample'],\n",
    "                                  subsample_freq = params['subsample_freq'],\n",
    "                                  learning_rate = params['learning_rate'],\n",
    "                                  feature_fraction = params['feature_fraction'],\n",
    "                                  max_depth = params['max_depth'],\n",
    "                                  lambda_l1 = params['lambda_l1'],\n",
    "                                  lambda_l2 = params['lambda_l2'],\n",
    "                                  early_stopping_rounds = params['early_stopping_rounds'])\n",
    "        \n",
    "        return clf.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                eval_set=[(val_set['X'], val_set['y'])])\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {\n",
    "            'n_estimators':5000,\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'multiclass',\n",
    "                    'eval_metric': 'logloss',\n",
    "                    'subsample': 0.7,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'feature_fraction': 0.9,\n",
    "                    'max_depth': 7,\n",
    "                    'lambda_l1': 1,  \n",
    "                    'lambda_l2': 1,\n",
    "                    'early_stopping_rounds': 100\n",
    "                    }\n",
    "        return params\n",
    "    \n",
    "gridParams = {\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 1000, 5000],\n",
    "    'num_leaves': [50, 70, 100],\n",
    "    'boosting_type' : ['gbdt'],\n",
    "    'objective' : ['binary', 'multiclass'],\n",
    "    'random_state' : [42, 501], # Updated from 'seed'\n",
    "#     'colsample_bytree' : [0.65, 0.66],\n",
    "    'subsample' : [0.7, 0.75],\n",
    "#     'reg_alpha' : [1,1.2],\n",
    "#     'reg_lambda' : [1,1.2,1.4],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Catb_Model(Base_Model):\n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        params = self.get_params()\n",
    "        clf = CatBoostClassifier(learning_rate = params['learning_rate'],\n",
    "                                  task_type = params['task_type'],\n",
    "                                  iterations = params['iterations'],\n",
    "                                  od_type = params['od_type'],\n",
    "                                  depth = params['depth'],\n",
    "                                  colsample_bylevel = params['colsample_bylevel'],\n",
    "                                  l2_leaf_reg = params['l2_leaf_reg'],\n",
    "                                  random_seed = params['random_seed'],\n",
    "                                  use_best_model = params['use_best_model'])\n",
    "        clf.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                eval_set=(val_set['X'], val_set['y']),\n",
    "                verbose=verbosity, \n",
    "                cat_features=self.categoricals)\n",
    "        return clf\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        params = {'learning_rate': 0.1,\n",
    "                   'task_type': \"CPU\",\n",
    "                   'iterations':1000,\n",
    "                   'od_type': \"Iter\",\n",
    "                    'depth': 10,\n",
    "                  'colsample_bylevel': 0.5, \n",
    "                   'early_stopping_rounds': 300,\n",
    "                    'l2_leaf_reg': 18,\n",
    "                   'random_seed': 42,\n",
    "                    'use_best_model': True\n",
    "                    }\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn_Model(Base_Model):\n",
    "    \n",
    "    def __init__(self, train_df, test_df, features, categoricals=[], n_splits=5, verbose=True):\n",
    "        features = features.copy()\n",
    "        if len(categoricals) > 0:\n",
    "            for cat in categoricals:\n",
    "                enc = OneHotEncoder()\n",
    "                train_cats = enc.fit_transform(train_df[[cat]])\n",
    "                test_cats = enc.transform(test_df[[cat]])\n",
    "                cat_cols = ['{}_{}'.format(cat, str(col)) for col in enc.active_features_]\n",
    "                features += cat_cols\n",
    "                train_cats = pd.DataFrame(train_cats.toarray(), columns=cat_cols)\n",
    "                test_cats = pd.DataFrame(test_cats.toarray(), columns=cat_cols)\n",
    "                train_df = pd.concat([train_df, train_cats], axis=1)\n",
    "                test_df = pd.concat([test_df, test_cats], axis=1)\n",
    "        scaler = MinMaxScaler()\n",
    "        train_df[features] = scaler.fit_transform(train_df[features])\n",
    "        test_df[features] = scaler.transform(test_df[features])\n",
    "        self.create_feat_2d(features)\n",
    "        super().__init__(train_df, test_df, features, categoricals, n_splits, verbose)\n",
    "        \n",
    "    def create_feat_2d(self, features, n_feats_repeat=50):\n",
    "        self.n_feats = len(features)\n",
    "        self.n_feats_repeat = n_feats_repeat\n",
    "        self.mask = np.zeros((self.n_feats_repeat, self.n_feats), dtype=np.int32)\n",
    "        for i in range(self.n_feats_repeat):\n",
    "            l = list(range(self.n_feats))\n",
    "            for j in range(self.n_feats):\n",
    "                c = l.pop(choice(range(len(l))))\n",
    "                self.mask[i, j] = c\n",
    "        self.mask = tf.convert_to_tensor(self.mask)\n",
    "        print(self.mask.shape)\n",
    "        \n",
    "    \n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "\n",
    "        inp = tf.keras.layers.Input(shape=(self.n_feats))\n",
    "        x = tf.keras.layers.Lambda(lambda x: tf.gather(x, self.mask, axis=1))(inp)\n",
    "        x = tf.keras.layers.Reshape((self.n_feats_repeat, self.n_feats, 1))(x)\n",
    "        x = tf.keras.layers.Conv2D(18, (50, 50), strides=50, activation='relu')(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        #x = tf.keras.layers.Dense(200, activation='relu')(x)\n",
    "        #x = tf.keras.layers.LayerNormalization()(x)\n",
    "        #x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(100, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        x = tf.keras.layers.Dense(50, activation='relu')(x)\n",
    "        x = tf.keras.layers.LayerNormalization()(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        out = tf.keras.layers.Dense(len(set(label)))(x)\n",
    "        \n",
    "        model = tf.keras.Model(inp, out)\n",
    "    \n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss='sparse_categorical_crossentropy')\n",
    "        print(model.summary())\n",
    "        save_best = tf.keras.callbacks.ModelCheckpoint('nn_model.w8', save_weights_only=True, save_best_only=True, verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(patience=20)\n",
    "        model.fit(train_set['X'], \n",
    "                train_set['y'], \n",
    "                validation_data=(val_set['X'], val_set['y']),\n",
    "                epochs=100,\n",
    "                 callbacks=[save_best, early_stop])\n",
    "        model.load_weights('nn_model.w8')\n",
    "        self.model = model\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, x_test):\n",
    "        return self.model.predict(x_test)\n",
    "        \n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "        \n",
    "    def get_params(self):\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affiliate_channel_api</th>\n",
       "      <th>affiliate_channel_content</th>\n",
       "      <th>affiliate_channel_direct</th>\n",
       "      <th>affiliate_channel_other</th>\n",
       "      <th>affiliate_channel_remarketing</th>\n",
       "      <th>affiliate_channel_sem-brand</th>\n",
       "      <th>affiliate_channel_sem-non-brand</th>\n",
       "      <th>affiliate_channel_seo</th>\n",
       "      <th>affiliate_provider_bing</th>\n",
       "      <th>affiliate_provider_craigslist</th>\n",
       "      <th>...</th>\n",
       "      <th>device_type=Tablet</th>\n",
       "      <th>device_type=other</th>\n",
       "      <th>device_type=iPad Tablet</th>\n",
       "      <th>device_type=-unknown-</th>\n",
       "      <th>device_type=Android Phone</th>\n",
       "      <th>secs_elapsed_mean</th>\n",
       "      <th>secs_elapsed_std</th>\n",
       "      <th>secs_elapsed_median</th>\n",
       "      <th>secs_elapsed_skew</th>\n",
       "      <th>country_destination</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>gxn3p5htnn</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820tgsjxq7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4ft3gnwmtx</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bjjt8pjhuk</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87mebub9p4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 157 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            affiliate_channel_api  affiliate_channel_content  \\\n",
       "id                                                             \n",
       "gxn3p5htnn                      0                          0   \n",
       "820tgsjxq7                      0                          0   \n",
       "4ft3gnwmtx                      0                          0   \n",
       "bjjt8pjhuk                      0                          0   \n",
       "87mebub9p4                      0                          0   \n",
       "\n",
       "            affiliate_channel_direct  affiliate_channel_other  \\\n",
       "id                                                              \n",
       "gxn3p5htnn                         1                        0   \n",
       "820tgsjxq7                         0                        0   \n",
       "4ft3gnwmtx                         1                        0   \n",
       "bjjt8pjhuk                         1                        0   \n",
       "87mebub9p4                         1                        0   \n",
       "\n",
       "            affiliate_channel_remarketing  affiliate_channel_sem-brand  \\\n",
       "id                                                                       \n",
       "gxn3p5htnn                              0                            0   \n",
       "820tgsjxq7                              0                            0   \n",
       "4ft3gnwmtx                              0                            0   \n",
       "bjjt8pjhuk                              0                            0   \n",
       "87mebub9p4                              0                            0   \n",
       "\n",
       "            affiliate_channel_sem-non-brand  affiliate_channel_seo  \\\n",
       "id                                                                   \n",
       "gxn3p5htnn                                0                      0   \n",
       "820tgsjxq7                                0                      1   \n",
       "4ft3gnwmtx                                0                      0   \n",
       "bjjt8pjhuk                                0                      0   \n",
       "87mebub9p4                                0                      0   \n",
       "\n",
       "            affiliate_provider_bing  affiliate_provider_craigslist  ...  \\\n",
       "id                                                                  ...   \n",
       "gxn3p5htnn                        0                              0  ...   \n",
       "820tgsjxq7                        0                              0  ...   \n",
       "4ft3gnwmtx                        0                              0  ...   \n",
       "bjjt8pjhuk                        0                              0  ...   \n",
       "87mebub9p4                        0                              0  ...   \n",
       "\n",
       "            device_type=Tablet  device_type=other  device_type=iPad Tablet  \\\n",
       "id                                                                           \n",
       "gxn3p5htnn                -1.0               -1.0                     -1.0   \n",
       "820tgsjxq7                -1.0               -1.0                     -1.0   \n",
       "4ft3gnwmtx                -1.0               -1.0                     -1.0   \n",
       "bjjt8pjhuk                -1.0               -1.0                     -1.0   \n",
       "87mebub9p4                -1.0               -1.0                     -1.0   \n",
       "\n",
       "            device_type=-unknown-  device_type=Android Phone  \\\n",
       "id                                                             \n",
       "gxn3p5htnn                   -1.0                       -1.0   \n",
       "820tgsjxq7                   -1.0                       -1.0   \n",
       "4ft3gnwmtx                   -1.0                       -1.0   \n",
       "bjjt8pjhuk                   -1.0                       -1.0   \n",
       "87mebub9p4                   -1.0                       -1.0   \n",
       "\n",
       "            secs_elapsed_mean  secs_elapsed_std  secs_elapsed_median  \\\n",
       "id                                                                     \n",
       "gxn3p5htnn               -1.0              -1.0                 -1.0   \n",
       "820tgsjxq7               -1.0              -1.0                 -1.0   \n",
       "4ft3gnwmtx               -1.0              -1.0                 -1.0   \n",
       "bjjt8pjhuk               -1.0              -1.0                 -1.0   \n",
       "87mebub9p4               -1.0              -1.0                 -1.0   \n",
       "\n",
       "            secs_elapsed_skew  country_destination  \n",
       "id                                                  \n",
       "gxn3p5htnn               -1.0                    7  \n",
       "820tgsjxq7               -1.0                    7  \n",
       "4ft3gnwmtx               -1.0                   10  \n",
       "bjjt8pjhuk               -1.0                   11  \n",
       "87mebub9p4               -1.0                   10  \n",
       "\n",
       "[5 rows x 157 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAINING_FINAL_CSV_FILE, index_col=0)\n",
    "test_df = pd.read_csv(TESTING_FINAL_CSV_FILE, index_col=0)\n",
    "label_df = pd.read_csv(LABELS_FINAL_CSV_FILE, index_col=0)\n",
    "\n",
    "features = train_df.columns\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(label_df[LABEL])\n",
    "label = encoder.transform(label_df[LABEL])\n",
    "train_df['country_destination'] = label\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "/home/leo/.local/lib/python3.6/site-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  \"timeout or by a memory leak.\", UserWarning\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed: 473.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.01, 'n_estimators': 1000, 'num_leaves': 30}\n",
      "0.23354826796091271\n"
     ]
    }
   ],
   "source": [
    "gridParams = {\n",
    "    'learning_rate': [0.01, 0.02, 0.05],\n",
    "    'n_estimators':[1000, 2000, 5000],\n",
    "    'num_leaves': [30, 40, 50]\n",
    "}\n",
    "\n",
    "params = {\n",
    "                    'n_estimators':5000,\n",
    "                    'boosting_type': 'gbdt',\n",
    "                    'objective': 'multiclass',\n",
    "#                     'eval_metric': 'logloss',\n",
    "                    'subsample': 0.7,\n",
    "                    'subsample_freq': 1,\n",
    "                    'learning_rate': 0.1,\n",
    "                    'colsample_bytree': 0.9,\n",
    "                    'max_depth': 7\n",
    "}\n",
    "\n",
    "clf = lgb.LGBMClassifier(max_bin = 100,\n",
    "                                 device = 'gpu',\n",
    "#                                  gpu_platform_id = 0,\n",
    "#                                  gpu_device_id = 0,\n",
    "                                 num_leaves = 70,\n",
    "                                 n_estimators = params['n_estimators'],\n",
    "                                  boosting_type = params['boosting_type'],\n",
    "                                  objective = params['objective'],\n",
    "                                  subsample = params['subsample'],\n",
    "                                  subsample_freq = params['subsample_freq'],\n",
    "                                  learning_rate = params['learning_rate'],\n",
    "                                  colsample_bytree = params['colsample_bytree'],\n",
    "                                  max_depth = params['max_depth'])\n",
    "\n",
    "grid = GridSearchCV(clf, gridParams,\n",
    "                    verbose=2,\n",
    "                    cv=5,\n",
    "                    n_jobs=-1)\n",
    "# Run the grid\n",
    "grid.fit(train_df[features], train_df[LABEL])\n",
    "\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1358.17403975, 1665.57387862, 1898.98262286, 2411.7192009 ,\n",
       "        2949.22355385, 3380.2995491 , 6138.64446907, 7133.81546345,\n",
       "        8395.57873259, 1223.11964297, 1612.25799899, 1888.23468132,\n",
       "        2207.02016158, 2261.68591266, 2905.66911588, 5526.35425978,\n",
       "        6595.138592  , 7943.64245973, 1284.56794624, 1592.88539581,\n",
       "        1773.91483011, 2099.89273272, 2701.34576964, 2911.96739721,\n",
       "        4708.7197381 , 4784.31993942, 4677.61622849]),\n",
       " 'std_fit_time': array([  3.06569977,  37.38548609,  37.02590575, 122.19469138,\n",
       "         81.70109807,  62.11349094, 128.29256524, 186.65759524,\n",
       "        258.595987  , 146.34199199,  34.40617609, 126.89808232,\n",
       "         81.69512509, 271.20486112, 206.14775665, 237.51947067,\n",
       "        132.93878988, 186.94126379,  60.98359078,  34.79737628,\n",
       "         49.21348593,  38.05747505, 137.68256854, 218.96221693,\n",
       "        240.8133388 , 142.41185045, 174.78905964]),\n",
       " 'mean_score_time': array([ 336.16026759,  422.21068225,  474.9230866 ,  761.9018784 ,\n",
       "         666.54806557,  864.90432258, 2195.9292902 , 2732.89766679,\n",
       "        2749.65369473,  356.80289936,  279.68127546,  411.21283298,\n",
       "        1041.63036637, 1014.43022718, 1077.14246826, 1897.77052388,\n",
       "        2299.4677175 , 2651.30431666,  244.85695481,  319.03168693,\n",
       "         526.5319314 ,  827.31903677, 1106.17314606, 1180.34471583,\n",
       "        1416.47732358, 1622.09811683, 1369.79851041]),\n",
       " 'std_score_time': array([ 25.58499702,  43.22501604,  46.20699709,  64.13638697,\n",
       "        105.75232463,  60.99215783, 196.64808214, 100.6333905 ,\n",
       "        158.0360197 ,  97.16695944,  16.24305645,  73.94283821,\n",
       "         42.49127129, 102.35100982,  75.97987536, 123.98436116,\n",
       "        174.64602941, 249.37057361,  28.75508995,  50.94736643,\n",
       "         51.0936705 ,  81.74551338, 112.88456901,  76.67639863,\n",
       "        195.2777492 ,  92.65594047,  37.9740317 ]),\n",
       " 'param_learning_rate': masked_array(data=[0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01,\n",
       "                    0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02, 0.02,\n",
       "                    0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[1000, 1000, 1000, 2000, 2000, 2000, 5000, 5000, 5000,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 5000, 5000, 5000,\n",
       "                    1000, 1000, 1000, 2000, 2000, 2000, 5000, 5000, 5000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_num_leaves': masked_array(data=[30, 40, 50, 30, 40, 50, 30, 40, 50, 30, 40, 50, 30, 40,\n",
       "                    50, 30, 40, 50, 30, 40, 50, 30, 40, 50, 30, 40, 50],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'learning_rate': 0.01, 'n_estimators': 1000, 'num_leaves': 30},\n",
       "  {'learning_rate': 0.01, 'n_estimators': 1000, 'num_leaves': 40},\n",
       "  {'learning_rate': 0.01, 'n_estimators': 1000, 'num_leaves': 50},\n",
       "  {'learning_rate': 0.01, 'n_estimators': 2000, 'num_leaves': 30},\n",
       "  {'learning_rate': 0.01, 'n_estimators': 2000, 'num_leaves': 40},\n",
       "  {'learning_rate': 0.01, 'n_estimators': 2000, 'num_leaves': 50},\n",
       "  {'learning_rate': 0.01, 'n_estimators': 5000, 'num_leaves': 30},\n",
       "  {'learning_rate': 0.01, 'n_estimators': 5000, 'num_leaves': 40},\n",
       "  {'learning_rate': 0.01, 'n_estimators': 5000, 'num_leaves': 50},\n",
       "  {'learning_rate': 0.02, 'n_estimators': 1000, 'num_leaves': 30},\n",
       "  {'learning_rate': 0.02, 'n_estimators': 1000, 'num_leaves': 40},\n",
       "  {'learning_rate': 0.02, 'n_estimators': 1000, 'num_leaves': 50},\n",
       "  {'learning_rate': 0.02, 'n_estimators': 2000, 'num_leaves': 30},\n",
       "  {'learning_rate': 0.02, 'n_estimators': 2000, 'num_leaves': 40},\n",
       "  {'learning_rate': 0.02, 'n_estimators': 2000, 'num_leaves': 50},\n",
       "  {'learning_rate': 0.02, 'n_estimators': 5000, 'num_leaves': 30},\n",
       "  {'learning_rate': 0.02, 'n_estimators': 5000, 'num_leaves': 40},\n",
       "  {'learning_rate': 0.02, 'n_estimators': 5000, 'num_leaves': 50},\n",
       "  {'learning_rate': 0.05, 'n_estimators': 1000, 'num_leaves': 30},\n",
       "  {'learning_rate': 0.05, 'n_estimators': 1000, 'num_leaves': 40},\n",
       "  {'learning_rate': 0.05, 'n_estimators': 1000, 'num_leaves': 50},\n",
       "  {'learning_rate': 0.05, 'n_estimators': 2000, 'num_leaves': 30},\n",
       "  {'learning_rate': 0.05, 'n_estimators': 2000, 'num_leaves': 40},\n",
       "  {'learning_rate': 0.05, 'n_estimators': 2000, 'num_leaves': 50},\n",
       "  {'learning_rate': 0.05, 'n_estimators': 5000, 'num_leaves': 30},\n",
       "  {'learning_rate': 0.05, 'n_estimators': 5000, 'num_leaves': 40},\n",
       "  {'learning_rate': 0.05, 'n_estimators': 5000, 'num_leaves': 50}],\n",
       " 'split0_test_score': array([0.12220374, 0.12140732, 0.12145417, 0.11955682, 0.11871355,\n",
       "        0.11758919, 0.11613689, 0.11395845, 0.11250615, 0.11883067,\n",
       "        0.11803425, 0.11801082, 0.11679277, 0.11494226, 0.11409899,\n",
       "        0.11070249, 0.10826638, 0.10690778, 0.11475487, 0.11325572,\n",
       "        0.11384132, 0.11035113, 0.10821953, 0.10763393, 0.1062519 ,\n",
       "        0.10554918, 0.10510412]),\n",
       " 'split1_test_score': array([0.11084563, 0.10709768, 0.10602015, 0.10864371, 0.10609042,\n",
       "        0.10501288, 0.10606699, 0.10442727, 0.10442727, 0.10780042,\n",
       "        0.10648864, 0.10684001, 0.10644179, 0.10555165, 0.10590302,\n",
       "        0.10426329, 0.10426329, 0.10705083, 0.10613727, 0.10433357,\n",
       "        0.10698056, 0.1056922 , 0.10646521, 0.10801124, 0.10789412,\n",
       "        0.11810729, 0.1245725 ]),\n",
       " 'split2_test_score': array([0.10119466, 0.0960178 , 0.0886156 , 0.07875381, 0.07535723,\n",
       "        0.07186695, 0.06615132, 0.06528461, 0.06364488, 0.07826189,\n",
       "        0.07322558, 0.07062544, 0.06448817, 0.06474584, 0.06348091,\n",
       "        0.06228625, 0.06146639, 0.06254392, 0.06488639, 0.06280159,\n",
       "        0.0622394 , 0.06280159, 0.0638557 , 0.06535488, 0.06608105,\n",
       "        0.07013352, 0.07418599]),\n",
       " 'split3_test_score': array([0.24855938, 0.24677911, 0.2472476 , 0.22131647, 0.22304989,\n",
       "        0.22328414, 0.20735535, 0.20594987, 0.20836261, 0.2227688 ,\n",
       "        0.22206606, 0.21637386, 0.2111033 , 0.2057859 , 0.2055048 ,\n",
       "        0.19826657, 0.20206137, 0.20634809, 0.21143125, 0.20089014,\n",
       "        0.21278988, 0.1983837 , 0.19763411, 0.20775357, 0.19948466,\n",
       "        0.20517686, 0.21084563]),\n",
       " 'split4_test_score': array([0.58493792, 0.58484423, 0.58498477, 0.58404779, 0.58402436,\n",
       "        0.58397751, 0.58376669, 0.58371984, 0.58369642, 0.58414149,\n",
       "        0.58411806, 0.58414149, 0.58383696, 0.58383696, 0.58381354,\n",
       "        0.58367299, 0.58367299, 0.58371984, 0.58376669, 0.58374327,\n",
       "        0.58376669, 0.58376669, 0.58393066, 0.58379011, 0.58369642,\n",
       "        0.58371984, 0.58381354]),\n",
       " 'mean_test_score': array([0.23354827, 0.23122923, 0.22966446, 0.22246372, 0.22144709,\n",
       "        0.22034613, 0.21589545, 0.21466801, 0.21452747, 0.22236065,\n",
       "        0.22078652, 0.21919832, 0.2165326 , 0.21497252, 0.21456025,\n",
       "        0.21183832, 0.21194608, 0.21331409, 0.21619529, 0.21300486,\n",
       "        0.21592357, 0.21219906, 0.21202104, 0.21450875, 0.21268163,\n",
       "        0.21653734, 0.21970436]),\n",
       " 'std_test_score': array([0.18366875, 0.18495242, 0.18625459, 0.18705171, 0.18797257,\n",
       "        0.18875552, 0.18965207, 0.1902031 , 0.19058839, 0.18734593,\n",
       "        0.18836545, 0.18874782, 0.18980385, 0.19010124, 0.1903438 ,\n",
       "        0.19110828, 0.1914512 , 0.19109933, 0.18996249, 0.19074281,\n",
       "        0.19038202, 0.19093962, 0.19099398, 0.19048632, 0.19059148,\n",
       "        0.18888778, 0.18762348]),\n",
       " 'rank_test_score': array([ 1,  2,  3,  4,  6,  8, 15, 17, 19,  5,  7, 10, 12, 16, 18, 27, 26,\n",
       "        21, 13, 22, 14, 24, 25, 20, 23, 11,  9], dtype=int32)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGB_WEIGHT = 0.4\n",
    "LGB_FRESH_WEIGHT = 0.6\n",
    "XGB_WEIGHT = 0.0\n",
    "XGB_FRESH_WEIGHT = 0.0\n",
    "CAT_WEIGHT = 0.0\n",
    "CAT_FRESH_WEIGHT = 0.0\n",
    "CNN_WEIGHT = 0.0\n",
    "CNN_FRESH_WEIGHT = 0.0\n",
    "FRESH_DATA_YEAR = 2014                   # Year when data is considered fresh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LGB_WEIGHT:\n",
    "    lgb_model = Lgb_Model(train_df, test_df, features=features, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LGB_FRESH_WEIGHT:\n",
    "    lgb_fresh_model = Lgb_Model(train_df[train_df[ACCOUNT_DATE_YEAR] >= FRESH_DATA_YEAR], test_df, features=features, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGB_WEIGHT:\n",
    "    xgb_model = Xgb_Model(train_df, test_df, features=features, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if XGB_FRESH_WEIGHT:\n",
    "    xgb_fresh_model = Xgb_Model(train_df[train_df[ACCOUNT_DATE_YEAR] >= FRESH_DATA_YEAR], test_df, features=features, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAT_WEIGHT:\n",
    "    cat_model = Catb_Model(train_df, test_df, features=features, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CAT_FRESH_WEIGHT:\n",
    "    cat_fresh_model = Catb_Model(train_df[train_df[ACCOUNT_DATE_YEAR] >= FRESH_DATA_YEAR], test_df, features=features, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CNN_WEIGHT:\n",
    "    cnn_model = Cnn_Model(train_df, test_df, features=features, verbose=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FRESH_DATA_YEAR = 1                   # Year when data is considered fresh, after normalize\n",
    "if CNN_FRESH_WEIGHT:\n",
    "    cnn_fresh_model = Cnn_Model(train_df[train_df[ACCOUNT_DATE_YEAR] >= FRESH_DATA_YEAR], test_df, features=features, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_df['country_destination'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions[0][np.argsort(predictions[0])[::-1]][4]\n",
    "# # predictions[0] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.zeros((len(test_df), len(set(label))))\n",
    "\n",
    "if LGB_WEIGHT:\n",
    "    predictions += LGB_WEIGHT * lgb_model.y_pred\n",
    "if LGB_FRESH_WEIGHT:\n",
    "    predictions += LGB_FRESH_WEIGHT * lgb_fresh_model.y_pred\n",
    "if XGB_WEIGHT:\n",
    "    predictions += XGB_WEIGHT * xgb_model.y_pred\n",
    "if XGB_FRESH_WEIGHT:\n",
    "    predictions += XGB_FRESH_WEIGHT * xgb_fresh_model.y_pred\n",
    "if CAT_WEIGHT:\n",
    "    predictions += CAT_WEIGHT * cat_model.y_pred\n",
    "if CAT_FRESH_WEIGHT:\n",
    "    predictions += CAT_FRESH_WEIGHT * cat_fresh_model.y_pred\n",
    "if CNN_WEIGHT:\n",
    "    predictions += CNN_WEIGHT * cnn_model.y_pred\n",
    "if CNN_FRESH_WEIGHT:\n",
    "    predictions += CNN_FRESH_WEIGHT * cnn_fresh_model.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Predict Users' new booking by combining of RnadomForest and XGB classifiers.\n",
    "# '''\n",
    "# DEPTH_XGB, ESTIMATORS_XGB, LEARNING_XGB, SUBSAMPLE_XGB, COLSAMPLE_XGB = (\n",
    "#     7, 60, 0.2, 0.7, 0.6)                # XGBoost parameters.\n",
    "\n",
    "# ESTIMATORS_RF, CRITERION_RF, DEPTH_RF, MIN_LEAF_RF, JOBS_RF = (\n",
    "#     500, 'gini', 20, 8, 30)              # RandomForestClassifier parameters.\n",
    "# FRESH_DATA_YEAR = 2014                   # Year when data is considered fresh.\n",
    "\n",
    "# # Tunning ensemble members. The votes show the importnce of each classfier\n",
    "# # in the final prediction.\n",
    "\n",
    "# XGB_ALL_VOTE, RF_ALL_VOTE, XGB_FRESH_VOTE, RF_FRESH_VOTE = (1, 1, 1, 1)\n",
    "\n",
    "\n",
    "# def perform_prediction(training, labels, testing, xgb_votes, rf_votes):\n",
    "#     \"\"\" Perform prediction using a combination of XGB and RandomForests. \"\"\"\n",
    "#     predictions = np.zeros((len(testing), len(set(labels))))\n",
    "#     # Predictions using xgboost.\n",
    "#     for i in range(xgb_votes):\n",
    "#         print('XGB vote %d' % i)\n",
    "#         clf = xgb.XGBClassifier(\n",
    "#             max_depth=DEPTH_XGB, learning_rate=LEARNING_XGB,\n",
    "#             n_estimators=ESTIMATORS_XGB, objective='multi:softprob',\n",
    "#             subsample=SUBSAMPLE_XGB, colsample_bytree=COLSAMPLE_XGB)\n",
    "#         clf.fit(training, labels, verbose=True)\n",
    "#         prediction = clf.predict_proba(testing)\n",
    "#         print(prediction)\n",
    "#         predictions += prediction\n",
    "#     # Predictions using RandomForestClassifier.\n",
    "# #     for i in range(rf_votes):\n",
    "# #         print('RandomForest vote %d' % i)\n",
    "# #         rand_forest = RandomForestClassifier(\n",
    "# #             n_estimators=ESTIMATORS_RF, criterion=CRITERION_RF, n_jobs=JOBS_RF,\n",
    "# #             max_depth=DEPTH_RF, min_samples_leaf=MIN_LEAF_RF, bootstrap=True)\n",
    "# #         rand_forest.fit(training, labels)\n",
    "# #         predictions += rand_forest.predict_proba(testing)\n",
    "#     return predictions\n",
    "\n",
    "# \"\"\" Perform prediction. \"\"\"\n",
    "# train_df = pd.read_csv(TRAINING_FINAL_CSV_FILE, index_col=0)\n",
    "# labels_df = pd.read_csv(LABELS_FINAL_CSV_FILE, index_col=0)\n",
    "# test_df = pd.read_csv(TESTING_FINAL_CSV_FILE, index_col=0)\n",
    "# assert set(train_df.index) == set(labels_df.index)\n",
    "\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(labels_df[LABEL])\n",
    "# predictions = np.zeros((len(test_df), len(encoder.classes_)))\n",
    "\n",
    "# # Use the full data set for the prediction.\n",
    "# labels = encoder.transform(labels_df[LABEL])\n",
    "# predictions += perform_prediction(\n",
    "#     train_df, labels, test_df, XGB_ALL_VOTE, RF_ALL_VOTE)\n",
    "\n",
    "# # Use only \"fresh\" data for prediction. Fresh data, are considered those\n",
    "# # that are an ACCOUNT_DATE_YEAR equal or higher than FRESH_DATA_YEAR.\n",
    "\n",
    "# train_fresh = train_df[train_df[ACCOUNT_DATE_YEAR] >= FRESH_DATA_YEAR]\n",
    "# labels_fresh = encoder.transform(labels_df.ix[train_fresh.index][LABEL])\n",
    "# predictions += perform_prediction(\n",
    "#     train_fresh, labels_fresh, test_df, XGB_FRESH_VOTE, RF_FRESH_VOTE)\n",
    "\n",
    "# print(\"Prediction shape %d %d\", predictions.shape[0], predictions.shape[1])\n",
    "\n",
    "# # Use the 5 classes with highest scores.\n",
    "# ids, countries = ([], [])\n",
    "# for i in range(len(test_df)):\n",
    "#     idx = test_df.index[i]\n",
    "#     ids += [idx] * 5\n",
    "#     countries += encoder.inverse_transform(\n",
    "#         np.argsort(predictions[i])[::-1])[:5].tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the 5 classes with highest scores.\n",
    "NUM_OF_CLASSES = 5\n",
    "ids, countries = ([], [])\n",
    "\n",
    "TOP_5 = ['NDF', 'US', 'other', 'FR', 'IT']\n",
    "count = 0\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    print('--------------------------------------- i %i -----------------------------------------' %i)\n",
    "    idx = test_df.index[i]\n",
    "    ids += [idx] * NUM_OF_CLASSES\n",
    "    \n",
    "#     print(encoder.inverse_transform(\n",
    "#         np.argsort(predictions[i])[::-1])[:5].tolist())\n",
    "    top_6 = encoder.inverse_transform(\n",
    "        np.argsort(predictions[i])[::-1])[:NUM_OF_CLASSES + 1].tolist()\n",
    "#     print(top_6)\n",
    "#     countries += encoder.inverse_transform(\n",
    "#         np.argsort(predictions[i])[::-1])[:NUM_OF_CLASSES + 1].tolist()\n",
    "    \n",
    "    fifth = predictions[i][np.argsort(predictions[i])[::-1]][NUM_OF_CLASSES - 1]\n",
    "    sixth = predictions[i][np.argsort(predictions[i])[::-1]][NUM_OF_CLASSES]\n",
    "#     print(predictions[i][np.argsort(predictions[i])[::-1]][NUM_OF_CLASSES - 1])\n",
    "#     print(predictions[i][np.argsort(predictions[i])[::-1]][NUM_OF_CLASSES])\n",
    "\n",
    "    if top_6[NUM_OF_CLASSES] in TOP_5 and top_6[NUM_OF_CLASSES - 1] not in TOP_5 and (fifth-sixth) < 0.002:\n",
    "        top_6.remove(top_6[NUM_OF_CLASSES - 1])\n",
    "        count += 1\n",
    "    else:\n",
    "        top_6.remove(top_6[NUM_OF_CLASSES])\n",
    "    print(top_6)    \n",
    "    \n",
    "    countries += top_6\n",
    "    \n",
    "print(count)\n",
    "print(count/len(test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMISSION_CSV = 'submission.csv'  # Where to store the predictions.\n",
    "\n",
    "# Save prediction in CSV file.\n",
    "sub = pd.DataFrame(\n",
    "    np.column_stack((ids, countries)), columns=['id', 'country'])\n",
    "print(sub.shape)\n",
    "sub.to_csv(SUBMISSION_CSV, index=False)\n",
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
